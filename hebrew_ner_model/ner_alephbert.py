{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-07-19T11:57:25.810663Z\",\"iopub.execute_input\":\"2021-07-19T11:57:25.810975Z\",\"iopub.status.idle\":\"2021-07-19T11:57:27.543509Z\",\"shell.execute_reply.started\":\"2021-07-19T11:57:25.81093Z\",\"shell.execute_reply\":\"2021-07-19T11:57:27.542483Z\"}}\n!git clone https://github.com/OnlpLab/NEMO-Corpus.git\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-07-19T11:57:27.546097Z\",\"iopub.execute_input\":\"2021-07-19T11:57:27.546493Z\",\"iopub.status.idle\":\"2021-07-19T11:57:34.762317Z\",\"shell.execute_reply.started\":\"2021-07-19T11:57:27.54645Z\",\"shell.execute_reply\":\"2021-07-19T11:57:34.761288Z\"}}\n!pip install transformers\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-07-19T11:57:34.766318Z\",\"iopub.execute_input\":\"2021-07-19T11:57:34.766653Z\",\"iopub.status.idle\":\"2021-07-19T11:58:06.375629Z\",\"shell.execute_reply.started\":\"2021-07-19T11:57:34.766618Z\",\"shell.execute_reply\":\"2021-07-19T11:58:06.374627Z\"}}\nfrom transformers import BertModel, BertTokenizerFast\ntokenizer = BertTokenizerFast.from_pretrained('onlplab/alephbert-base')\nfrom transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"onlplab/alephbert-base\", num_labels=135)\nmodel.save_pretrained(\"./initial_pretrained\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-07-19T11:58:06.377438Z\",\"iopub.execute_input\":\"2021-07-19T11:58:06.377804Z\",\"iopub.status.idle\":\"2021-07-19T11:58:07.084314Z\",\"shell.execute_reply.started\":\"2021-07-19T11:58:06.377763Z\",\"shell.execute_reply\":\"2021-07-19T11:58:07.083144Z\"}}\n!ls -latr ./initial_pretrained\n\n# %% [code]\ndataset = {\n            \"name\": \"NEMO Corpus\",\n            \"train_path\": \"./NEMO-Corpus/data/spmrl/gold/token-multi_gold_train.bmes\",\n            \"dev_path\": \"./NEMO-Corpus/data/spmrl/gold/token-multi_gold_dev.bmes\",\n            \"test_path\": \"./NEMO-Corpus/data/spmrl/gold/token-multi_gold_test.bmes\",\n            'classes': []\n          }\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-07-19T11:58:10.426892Z\",\"iopub.execute_input\":\"2021-07-19T11:58:10.427232Z\",\"iopub.status.idle\":\"2021-07-19T11:58:11.247522Z\",\"shell.execute_reply.started\":\"2021-07-19T11:58:10.4272Z\",\"shell.execute_reply\":\"2021-07-19T11:58:11.246431Z\"}}\n!cat NEMO-Corpus/data/spmrl/gold/token-multi_gold_train.bmes NEMO-Corpus/data/spmrl/gold/token-multi_gold_dev.bmes NEMO-Corpus/data/spmrl/gold/token-multi_gold_test.bmes | cut -d \" \" -f 2 | grep -v \"^$\"| sort | uniq > labels.txt\n\n# %% [code]\nlabels = []\nwith open('labels.txt', 'r') as file:\n    for line in file:\n        labels.append(line.strip())\nlabels.extend([\"OCC\", \"O^OCC\", \"O^O^OCC\", \"O^O^O^OCC\"])\nprint(labels)\ndataset['classes'] = labels\nprint(len(labels))\nprint(labels[61])\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-07-19T11:58:17.782569Z\",\"iopub.execute_input\":\"2021-07-19T11:58:17.78293Z\",\"iopub.status.idle\":\"2021-07-19T11:58:18.090415Z\",\"shell.execute_reply.started\":\"2021-07-19T11:58:17.782883Z\",\"shell.execute_reply\":\"2021-07-19T11:58:18.089386Z\"}}\nimport pandas as pd\n\ndef read_data():\n    train = pd.read_csv(dataset['train_path'], sep=' ', engine='python',quoting=3, encoding='utf-8', error_bad_lines=False, names=['word', 'label'])\n    dev = pd.read_csv(dataset['dev_path'], sep=' ', engine='python',quoting=3, encoding='utf-8', error_bad_lines=False, names=['word', 'label'])\n    test = pd.read_csv(dataset['test_path'], sep=' ', engine='python',quoting=3, encoding='utf-8', error_bad_lines=False, names=['word', 'label'])\n    return train, dev, test\ntrain, dev, test = read_data()\n# train.to_csv(\"train_example.csv\")\n\n# %% [code]\noccupation_file_path = '../input/occupations/occupations.txt'\nocc_file = open(occupation_file_path, 'r')\noccupations_set = set([line.replace('\\n', '') for line in occ_file.readlines() if line.replace('\\n', '')])\n\n# %% [code]\nlabel_not_ne = [\"O\", \"O^O\", \"O^O^O\", \"O^O^O^O\"]\n\n# %% [code]\ndef is_word_an_occupation(word, label):\n    if label in label_not_ne:\n        num_prefixes = label.count(\"^\", 0, len(label))\n        if word[num_prefixes:] in occupations_set:\n            return True\n    return False\n# print(is_word_an_occupation(\"\", \"O^O^O\"))\n\ndef replace_occupations_label(dataframe):\n    for index, row in dataframe.iterrows():\n        if is_word_an_occupation(row[\"word\"], row[\"label\"]):\n            row[\"label\"] = row[\"label\"][:-1] + \"OCC\"\n# replace_occupations_label(train)\n# testing_df = train[:10]\n# replace_occupations_label(testing_df)\n# print(testing_df)\n# testing_df.loc[testing_df['word'] in (\"转\",\"爪\"), 'label'] = 'J'\n# np.where(testing_df['word'] in (\"转\",\"爪\"), 'J', testing_df[\"label\"])\n# testing_df[\"label\"] = ['J' if word in (\"转\",\"爪\") else '0' for word in testing_df['word']]\n# print(testing_df)\nreplace_occupations_label(train)\nreplace_occupations_label(test)\nreplace_occupations_label(dev)\n\n# for r in label_not_ne:\n#     print(r.count(\"^\", 0, len(r)))\n\ndef change_label_example():\n    train[\"label\"][0] = \"O\"\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-07-19T11:58:37.06152Z\",\"iopub.execute_input\":\"2021-07-19T11:58:37.061875Z\",\"iopub.status.idle\":\"2021-07-19T11:58:37.100641Z\",\"shell.execute_reply.started\":\"2021-07-19T11:58:37.061843Z\",\"shell.execute_reply\":\"2021-07-19T11:58:37.099805Z\"}}\ntrain[train[\"label\"] == 'O^O^OCC']\n\n# %% [code]\nfrom sklearn import preprocessing\n\nlabel_encoder = preprocessing.LabelEncoder()\nlabel_encoder.fit(labels)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-07-19T11:58:46.541693Z\",\"iopub.execute_input\":\"2021-07-19T11:58:46.542023Z\",\"iopub.status.idle\":\"2021-07-19T11:58:46.5513Z\",\"shell.execute_reply.started\":\"2021-07-19T11:58:46.541992Z\",\"shell.execute_reply\":\"2021-07-19T11:58:46.550284Z\"}}\nlabel_encoder.inverse_transform([61])\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-07-19T11:58:48.883781Z\",\"iopub.execute_input\":\"2021-07-19T11:58:48.884114Z\",\"iopub.status.idle\":\"2021-07-19T11:58:51.389112Z\",\"shell.execute_reply.started\":\"2021-07-19T11:58:48.884085Z\",\"shell.execute_reply\":\"2021-07-19T11:58:51.388079Z\"}}\ntrain_encodings = tokenizer(train[\"word\"].to_list()[:60000], truncation=True, padding=True)\ndev_encodings = tokenizer(dev[\"word\"].to_list(), truncation=True, padding=True)\ntest_encodings = tokenizer(test[\"word\"].to_list(), truncation=True, padding=True)\ntrain_labels=label_encoder.transform(train[\"label\"].to_list()[:60000])\ndev_labels=label_encoder.transform(dev[\"label\"].to_list())\ntest_labels=label_encoder.transform(test[\"label\"].to_list())\nprint(dev_labels)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-07-19T11:58:53.267437Z\",\"iopub.execute_input\":\"2021-07-19T11:58:53.267789Z\",\"iopub.status.idle\":\"2021-07-19T11:58:53.27407Z\",\"shell.execute_reply.started\":\"2021-07-19T11:58:53.267759Z\",\"shell.execute_reply\":\"2021-07-19T11:58:53.273004Z\"}}\nprint(len(train_encodings['input_ids']))\nprint(train_labels)\nprint(test_labels)\nprint(dev_labels)\n# print(train[\"word\"].to_list())\n\n# %% [code]\nimport torch\n\nclass HebrewNERDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\ntrain_dataset = HebrewNERDataset(train_encodings, train_labels)\ndev_dataset = HebrewNERDataset(dev_encodings, dev_labels)\ntest_dataset = HebrewNERDataset(test_encodings, test_labels)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-07-17T14:46:12.04107Z\",\"iopub.execute_input\":\"2021-07-17T14:46:12.043824Z\",\"iopub.status.idle\":\"2021-07-17T14:46:12.060645Z\",\"shell.execute_reply.started\":\"2021-07-17T14:46:12.043774Z\",\"shell.execute_reply\":\"2021-07-17T14:46:12.059324Z\"}}\ntrain_dataset.__getitem__(2)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-07-17T14:46:13.726625Z\",\"iopub.execute_input\":\"2021-07-17T14:46:13.726986Z\",\"iopub.status.idle\":\"2021-07-17T14:46:19.678281Z\",\"shell.execute_reply.started\":\"2021-07-17T14:46:13.726955Z\",\"shell.execute_reply\":\"2021-07-17T14:46:19.677312Z\"}}\n!pip install wandb\n\n# %% [code]\nCUDA_LAUNCH_BLOCKING=1\nfrom transformers import Trainer,TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir='./results',          # output directory\n    num_train_epochs=3,              # total number of training epochs\n    per_device_train_batch_size=16,  # batch size per device during training\n    per_device_eval_batch_size=64,   # batch size for evaluation\n    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n    weight_decay=0.01,               # strength of weight decay\n    logging_dir='./logs',            # directory for storing logs\n    logging_steps=10\n)\n\ntrainer = Trainer(\n    model=model,                         # the instantiated  Transformers model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=dev_dataset             # evaluation dataset\n)\n\ntrainer.train()\ntrainer.save_model(\"./alephbert_ner\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-07-17T15:03:40.047085Z\",\"iopub.execute_input\":\"2021-07-17T15:03:40.047423Z\",\"iopub.status.idle\":\"2021-07-17T15:03:40.771479Z\",\"shell.execute_reply.started\":\"2021-07-17T15:03:40.047393Z\",\"shell.execute_reply\":\"2021-07-17T15:03:40.770532Z\"}}\n!ls -latr ./alephbert_ner/\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-07-17T15:03:53.757575Z\",\"iopub.execute_input\":\"2021-07-17T15:03:53.758064Z\",\"iopub.status.idle\":\"2021-07-17T15:04:05.965373Z\",\"shell.execute_reply.started\":\"2021-07-17T15:03:53.758029Z\",\"shell.execute_reply\":\"2021-07-17T15:04:05.964418Z\"}}\nraw_pred, _, _ = trainer.predict(test_dataset)\n\n# Preprocess raw predictions\ny_pred = np.argmax(raw_pred, axis=1)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-07-17T15:03:50.580511Z\",\"iopub.execute_input\":\"2021-07-17T15:03:50.580899Z\",\"iopub.status.idle\":\"2021-07-17T15:03:50.592569Z\",\"shell.execute_reply.started\":\"2021-07-17T15:03:50.580865Z\",\"shell.execute_reply\":\"2021-07-17T15:03:50.588848Z\"}}\nimport numpy as np\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-07-17T08:42:43.08002Z\",\"iopub.execute_input\":\"2021-07-17T08:42:43.080355Z\",\"iopub.status.idle\":\"2021-07-17T08:42:43.088822Z\",\"shell.execute_reply.started\":\"2021-07-17T08:42:43.080322Z\",\"shell.execute_reply\":\"2021-07-17T08:42:43.087491Z\"}}\ny_pred\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-07-17T07:47:08.707508Z\",\"iopub.execute_input\":\"2021-07-17T07:47:08.707861Z\",\"iopub.status.idle\":\"2021-07-17T07:47:08.720714Z\",\"shell.execute_reply.started\":\"2021-07-17T07:47:08.707826Z\",\"shell.execute_reply\":\"2021-07-17T07:47:08.719682Z\"}}\ntest_dataset.labels\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-07-17T15:04:08.336533Z\",\"iopub.execute_input\":\"2021-07-17T15:04:08.33692Z\",\"iopub.status.idle\":\"2021-07-17T15:04:08.432179Z\",\"shell.execute_reply.started\":\"2021-07-17T15:04:08.336887Z\",\"shell.execute_reply\":\"2021-07-17T15:04:08.430992Z\"}}\nfrom sklearn.metrics import f1_score, recall_score, precision_score\ncount_equals=0\nfor a,b in zip(test_dataset.labels, y_pred):\n    if a==b:\n        count_equals+=1\nprint(f\"accuracy={count_equals/len(y_pred)}\")\n\ndef evaluate(y_test, predicted):\n    print(\"Recall Macro: \" + str(recall_score(y_test, predicted, average='macro')))\n    print(\"Precision Macro: \" + str(precision_score(y_test, predicted, average='macro')))\n    print(\"F1 Macro: \" + str(f1_score(y_test, predicted, average='macro')))\n    print(\"Recall Micro: \" + str(recall_score(y_test, predicted, average='micro')))\n    print(\"Precision Micro: \" + str(precision_score(y_test, predicted, average='micro')))\n    print(\"F1 Micro: \" + str(f1_score(y_test, predicted, average='micro')))\n    print(\"F1: \" + str(f1_score(y_test, predicted, average='weighted')))\n    \nevaluate(test_dataset.labels, y_pred)\nres = (list(filter(lambda x: x[1] != 61, list(zip(list(test_dataset.labels), list(y_pred))))))\ntest_no_o, pred_no_o = list(zip(*res))\nprint(\"F1: \" + str(f1_score(test_no_o, pred_no_o, average='micro')))\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-07-17T15:04:14.429911Z\",\"iopub.execute_input\":\"2021-07-17T15:04:14.430234Z\",\"iopub.status.idle\":\"2021-07-17T15:04:14.905132Z\",\"shell.execute_reply.started\":\"2021-07-17T15:04:14.430204Z\",\"shell.execute_reply\":\"2021-07-17T15:04:14.90432Z\"}}\ntest_sent = \"专 砖   专  专  砖抓\"\ntest_sent = test_sent.split(\" \")\ntest_sent = tokenizer(test_sent, truncation=True, padding=True)\ntest_sent = HebrewNERDataset(test_sent, [0, 0, 0, 0, 0, 0])\ntest_sent_pred, _, _ = trainer.predict(test_sent)\ntest_sent_pred = np.argmax(test_sent_pred, axis=1)\nlabel_encoder.inverse_transform(test_sent_pred)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-07-17T15:11:18.215907Z\",\"iopub.execute_input\":\"2021-07-17T15:11:18.216227Z\",\"iopub.status.idle\":\"2021-07-17T15:11:18.686283Z\",\"shell.execute_reply.started\":\"2021-07-17T15:11:18.216198Z\",\"shell.execute_reply\":\"2021-07-17T15:11:18.685479Z\"}}\ntest_s = \"专 砖  专 砖 专抓 砖转 1879  注 驻专住 砖 驻专住 \"\n# test_s = \"专 砖   专  专  砖抓\"\ntest_s = test_s.split(\" \")\ntest_s_tokenized = tokenizer(test_s, truncation=True, padding=True)\ntest_s_dataset = HebrewNERDataset(test_s_tokenized, [0 for i in range(len(test_s))])\ntest_s_pred, _, _ = trainer.predict(test_s_dataset)\ntest_s_pred = np.argmax(test_s_pred, axis=1)\nlabel_encoder.inverse_transform(test_s_pred)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-07-17T15:04:26.41412Z\",\"iopub.execute_input\":\"2021-07-17T15:04:26.414433Z\",\"iopub.status.idle\":\"2021-07-17T15:04:26.885386Z\",\"shell.execute_reply.started\":\"2021-07-17T15:04:26.414403Z\",\"shell.execute_reply\":\"2021-07-17T15:04:26.884603Z\"}}\ntest_s = \"专 砖  驻专住 \"\ntest_s = test_s.split(\" \")\ntest_s_tokenized = tokenizer(test_s, truncation=True, padding=True)\ntest_s_dataset = HebrewNERDataset(test_s_tokenized, [0 for i in range(len(test_s))])\ntest_s_pred, _, _ = trainer.predict(test_s_dataset)\ntest_s_pred = np.argmax(test_s_pred, axis=1)\nlabel_encoder.inverse_transform(test_s_pred)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-07-17T15:04:28.786615Z\",\"iopub.execute_input\":\"2021-07-17T15:04:28.786958Z\",\"iopub.status.idle\":\"2021-07-17T15:04:29.26459Z\",\"shell.execute_reply.started\":\"2021-07-17T15:04:28.786929Z\",\"shell.execute_reply\":\"2021-07-17T15:04:29.263733Z\"}}\ntest_s = '专  住 转 专转  注  住 砖专'\ntest_s = test_s.split(\" \")\ntest_s_tokenized = tokenizer(test_s, truncation=True, padding=True)\ntest_s_dataset = HebrewNERDataset(test_s_tokenized, [0 for i in range(len(test_s))])\ntest_s_pred, _, _ = trainer.predict(test_s_dataset)\ntest_s_pred = np.argmax(test_s_pred, axis=1)\nlabel_encoder.inverse_transform(test_s_pred)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-07-17T15:09:27.314584Z\",\"iopub.execute_input\":\"2021-07-17T15:09:27.314955Z\",\"iopub.status.idle\":\"2021-07-17T15:09:27.821761Z\",\"shell.execute_reply.started\":\"2021-07-17T15:09:27.314924Z\",\"shell.execute_reply\":\"2021-07-17T15:09:27.82098Z\"}}\ntest_s = '   住驻专  专'\ntest_s = test_s.split(\" \")\ntest_s_tokenized = tokenizer(test_s, truncation=True, padding=True)\ntest_s_dataset = HebrewNERDataset(test_s_tokenized, [0 for i in range(len(test_s))])\ntest_s_pred, _, _ = trainer.predict(test_s_dataset)\ntest_s_pred = np.argmax(test_s_pred, axis=1)\nlabel_encoder.inverse_transform(test_s_pred)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-07-17T15:04:33.310345Z\",\"iopub.execute_input\":\"2021-07-17T15:04:33.310707Z\",\"iopub.status.idle\":\"2021-07-17T15:04:33.789913Z\",\"shell.execute_reply.started\":\"2021-07-17T15:04:33.310659Z\",\"shell.execute_reply\":\"2021-07-17T15:04:33.789095Z\"}}\ntest_s = \"专 专  专转 \"\ntest_s = test_s.split(\" \")\ntest_s_tokenized = tokenizer(test_s, truncation=True, padding=True)\ntest_s_dataset = HebrewNERDataset(test_s_tokenized, [0 for i in range(len(test_s))])\ntest_s_pred, _, _ = trainer.predict(test_s_dataset)\ntest_s_pred = np.argmax(test_s_pred, axis=1)\nlabel_encoder.inverse_transform(test_s_pred)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-07-17T15:11:40.748109Z\",\"iopub.execute_input\":\"2021-07-17T15:11:40.74864Z\",\"iopub.status.idle\":\"2021-07-17T15:11:41.358964Z\",\"shell.execute_reply.started\":\"2021-07-17T15:11:40.748582Z\",\"shell.execute_reply\":\"2021-07-17T15:11:41.358126Z\"}}\ntest_s = \"住 砖抓   拽专\"\ntest_s = test_s.split(\" \")\ntest_s_tokenized = tokenizer(test_s, truncation=True, padding=True)\ntest_s_dataset = HebrewNERDataset(test_s_tokenized, [0 for i in range(len(test_s))])\ntest_s_pred, _, _ = trainer.predict(test_s_dataset)\ntest_s_pred = np.argmax(test_s_pred, axis=1)\nlabel_encoder.inverse_transform(test_s_pred)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-07-17T15:12:12.244861Z\",\"iopub.execute_input\":\"2021-07-17T15:12:12.24519Z\",\"iopub.status.idle\":\"2021-07-17T15:12:12.71718Z\",\"shell.execute_reply.started\":\"2021-07-17T15:12:12.24516Z\",\"shell.execute_reply\":\"2021-07-17T15:12:12.716234Z\"}}\ntest_s = \"爪专转 专    \"\ntest_s = test_s.split(\" \")\ntest_s_tokenized = tokenizer(test_s, truncation=True, padding=True)\ntest_s_dataset = HebrewNERDataset(test_s_tokenized, [0 for i in range(len(test_s))])\ntest_s_pred, _, _ = trainer.predict(test_s_dataset)\ntest_s_pred = np.argmax(test_s_pred, axis=1)\nlabel_encoder.inverse_transform(test_s_pred)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-07-17T15:06:08.610255Z\",\"iopub.execute_input\":\"2021-07-17T15:06:08.610585Z\",\"iopub.status.idle\":\"2021-07-17T15:06:34.197618Z\",\"shell.execute_reply.started\":\"2021-07-17T15:06:08.610555Z\",\"shell.execute_reply\":\"2021-07-17T15:06:34.196818Z\"}}\nimport shutil\nshutil.make_archive('alephbert_ner_occ_morph', 'zip', '/kaggle/working/alephbert_ner')\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-07-17T15:06:46.900196Z\",\"iopub.execute_input\":\"2021-07-17T15:06:46.900515Z\",\"iopub.status.idle\":\"2021-07-17T15:06:46.907651Z\",\"shell.execute_reply.started\":\"2021-07-17T15:06:46.900486Z\",\"shell.execute_reply\":\"2021-07-17T15:06:46.90675Z\"}}\nfrom IPython.display import FileLink\nFileLink('./alephbert_ner_occ_morph.zip')","metadata":{"_uuid":"20962903-8ba4-47e0-babf-a9c2974a916b","_cell_guid":"3a8daf21-1037-4d6c-96e5-710ba481bec4","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}