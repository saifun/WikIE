{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "opposite-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import stanza\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62972403",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEAD = 'head'\n",
    "POS = 'pos'\n",
    "Info = namedtuple('Info', [HEAD, POS])\n",
    "WordNerInfo = namedtuple('WordNerInfo', ['text', 'ner_definition', 'root'])\n",
    "ROOT = -1\n",
    "SINGLETON = 'S-'\n",
    "BEGIN = 'B-'\n",
    "INSIDE = 'I-'\n",
    "OUTSIDE = 'O'\n",
    "END = 'E-'\n",
    "ner_translator = {\n",
    "    'GPE': 'מדינה',\n",
    "    'PER': 'שם',\n",
    "    'ORG': 'ארגון',\n",
    "    'LOC': 'מיקום',\n",
    "    'DUC': 'מוצר',\n",
    "    'EVE': 'אירוע',\n",
    "    'ANG': 'שפה',\n",
    "    'FAC': 'מתקן',\n",
    "    'WOA': 'יצירת אומנות',\n",
    "    'OCC': 'מקצוע',\n",
    "    'PRI': 'פרס',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "valuable-richmond",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Processor:\n",
    "    def __init__(self):\n",
    "        self.heb_nlp = stanza.Pipeline(lang='he', processors='tokenize,mwt,pos,lemma,depparse')\n",
    "        #replace MY_TOKEN with the token you got from the langndata website\n",
    "        self.yap_token=\"21e79c010599d991fd815b93048b245b\"\n",
    "    \n",
    "    def get_stanza_analysis(self, text):\n",
    "        text += \" XX\"\n",
    "        doc=self.heb_nlp(text)\n",
    "        lst=[]\n",
    "        for sen in doc.sentences:\n",
    "            for token in sen.tokens:\n",
    "                for word in token.words:\n",
    "                    features=[(word.text,\n",
    "                               word.lemma,\n",
    "                               word.upos,\n",
    "                               word.xpos,\n",
    "                               word.head,\n",
    "                               word.deprel,\n",
    "                               word.feats)]\n",
    "\n",
    "                    df=pd.DataFrame(features, columns=[\"text\", \"lemma\", \"upos\", \"xpos\", \"head\", \"deprel\",\"feats\"])\n",
    "                    lst.append(df)\n",
    "        tot_df=pd.concat(lst, ignore_index=True)\n",
    "        tot_df=tot_df.shift(1).iloc[1:]\n",
    "        tot_df[\"head\"]=tot_df[\"head\"].astype(int)\n",
    "#         print(tot_df.head(50))\n",
    "        return tot_df['text'], tot_df['head'], tot_df['upos']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0e527bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_verb_name_for_verb(verb):\n",
    "    verb_names_db = {\n",
    "        'נולד': 'לידה',\n",
    "        'גר': 'מגורים',\n",
    "        'יסד': 'יסוד',\n",
    "        'הקים': 'הקמה',\n",
    "        'עבד': 'עבודה',\n",
    "        'זכה': 'זכיה',\n",
    "    }\n",
    "    return verb_names_db[verb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "proud-palestinian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stanza.download('he')\n",
    "# stanza_nlp = stanza.Pipeline('he')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "invisible-letter",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 17:53:15 INFO: Loading these models for language: he (Hebrew):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | htb     |\n",
      "| mwt       | htb     |\n",
      "| pos       | htb     |\n",
      "| lemma     | htb     |\n",
      "| depparse  | htb     |\n",
      "=======================\n",
      "\n",
      "2021-08-10 17:53:15 INFO: Use device: cpu\n",
      "2021-08-10 17:53:15 INFO: Loading: tokenize\n",
      "2021-08-10 17:53:15 INFO: Loading: mwt\n",
      "2021-08-10 17:53:15 INFO: Loading: pos\n",
      "2021-08-10 17:53:15 INFO: Loading: lemma\n",
      "2021-08-10 17:53:15 INFO: Loading: depparse\n",
      "2021-08-10 17:53:16 INFO: Done loading processors!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1        אלברט\n",
       " 2    איינשטיין\n",
       " 3         נולד\n",
       " 4            ב\n",
       " 5       גרמניה\n",
       " 6            ו\n",
       " 7           גר\n",
       " 8            ב\n",
       " 9        שוויץ\n",
       " Name: text, dtype: object,\n",
       " 1    3\n",
       " 2    1\n",
       " 3    0\n",
       " 4    5\n",
       " 5    3\n",
       " 6    7\n",
       " 7    3\n",
       " 8    9\n",
       " 9    7\n",
       " Name: head, dtype: int64,\n",
       " 1    PROPN\n",
       " 2    PROPN\n",
       " 3     VERB\n",
       " 4      ADP\n",
       " 5    PROPN\n",
       " 6    CCONJ\n",
       " 7     VERB\n",
       " 8      ADP\n",
       " 9    PROPN\n",
       " Name: upos, dtype: object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"\"\"\n",
    "אלברט איינשטיין נולד בגרמניה וגר בשוויץ\n",
    "\"\"\"\n",
    "processor=Processor()\n",
    "processor.get_stanza_analysis(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "governing-redhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticTree:\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "        self.processor = Processor()\n",
    "        \n",
    "    def parse_text(self):\n",
    "        parsed_text, tree, pos = self.processor.get_stanza_analysis(self.text)\n",
    "        word_list = list(zip(list(parsed_text), map(lambda head: head - 1, list(tree)), list(pos)))\n",
    "        self.tree = {word: Info(head, pos) for word, head, pos in word_list}\n",
    "        self.parsed_text = parsed_text\n",
    "        print('PARSED_TEXT', self.parsed_text, type(self.parsed_text))\n",
    "        \n",
    "    def __str__(self):\n",
    "        tree_rep = '{\\n'\n",
    "        for word, info in self.tree.items():\n",
    "            tree_rep += '{}: {}\\n'.format(word, info)\n",
    "        tree_rep += '}\\n'\n",
    "        return tree_rep\n",
    "        \n",
    "    def is_verb(self, word):\n",
    "        return self.tree[word].pos == 'VERB'\n",
    "    \n",
    "    def is_root(self, word):\n",
    "        return self.tree[word].head == ROOT\n",
    "    \n",
    "    def get_word_in_index(self, index):\n",
    "        return list(self.tree.keys())[index]\n",
    "        \n",
    "    def find_verb_root(self, word):\n",
    "        while(not self.is_root(word)):\n",
    "            if self.is_verb(word):\n",
    "                return word\n",
    "            word = self.get_word_in_index(self.tree[word].head)\n",
    "        return word\n",
    "    \n",
    "    def build_ner_for_text(self, ner):\n",
    "        self.ner = ner\n",
    "        \n",
    "    def cluster_text_by_ner(self):\n",
    "        text_with_ner = list(zip(self.parsed_text, self.ner))\n",
    "        self.clustered_text = []\n",
    "        for text, ner in text_with_ner:\n",
    "            ner = ner.split('^')[-1]\n",
    "            if ner.startswith(SINGLETON) or ner.startswith(BEGIN):\n",
    "                self.clustered_text.append((text, ner[2:]))\n",
    "            elif ner.startswith(OUTSIDE):\n",
    "                self.clustered_text.append((text, OUTSIDE))\n",
    "            else:  # INSIDE or END\n",
    "                previous_text, previous_ner = self.clustered_text.pop()\n",
    "                united_text = '{} {}'.format(previous_text, text)\n",
    "                self.clustered_text.append((united_text, previous_ner))\n",
    "                \n",
    "                \n",
    "    def _get_info_for_word_cluster(self, word_cluster):\n",
    "        text, ner = word_cluster\n",
    "        root = self.find_verb_root(text.split(' ')[0])\n",
    "#         if self.is_verb(root):\n",
    "#             root = get_verb_name_for_verb(root)\n",
    "        ner_definition = ner_translator[ner]\n",
    "        return WordNerInfo(text, ner_definition, root)\n",
    "    \n",
    "    def _is_word_interesting(self, ner):\n",
    "        return ner != OUTSIDE\n",
    "        \n",
    "    def get_interesting_words_info(self):\n",
    "        \"\"\"\n",
    "        In the future - think what happens when there is more than one name (אלברט דיבר עם ניקו).\n",
    "        We need to take into account also the POS of the word.\n",
    "        \"\"\"\n",
    "        interesting_words = list(filter(lambda ner_word: \n",
    "                                        self._is_word_interesting(ner_word[1]), self.clustered_text))\n",
    "        interesting_roots = [self._get_info_for_word_cluster(word) for word in interesting_words]\n",
    "        return interesting_roots\n",
    "    \n",
    "    def build_info_dict(self, interesting_words_info):\n",
    "        return {\n",
    "            '{}{}'.format(word_info.ner_definition, \n",
    "                             ' - ' + word_info.root if word_info.ner_definition != 'שם' else ''): word_info.text\n",
    "            for word_info in interesting_words_info\n",
    "        }\n",
    "    \n",
    "    def add_date_tags(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ccfa19d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_interesting_words(words):\n",
    "    repr_str = '[\\n'\n",
    "    for word in words:\n",
    "        repr_str += 'טקסט: {}, NER: {}, שורש: {}\\n'.format(word.text, word.ner_definition, word.root)\n",
    "    repr_str += ']\\n'\n",
    "    return repr_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0a9c3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 17:54:04 INFO: Loading these models for language: he (Hebrew):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | htb     |\n",
      "| mwt       | htb     |\n",
      "| pos       | htb     |\n",
      "| lemma     | htb     |\n",
      "| depparse  | htb     |\n",
      "=======================\n",
      "\n",
      "2021-08-10 17:54:04 INFO: Use device: cpu\n",
      "2021-08-10 17:54:04 INFO: Loading: tokenize\n",
      "2021-08-10 17:54:04 INFO: Loading: mwt\n",
      "2021-08-10 17:54:04 INFO: Loading: pos\n",
      "2021-08-10 17:54:05 INFO: Loading: lemma\n",
      "2021-08-10 17:54:05 INFO: Loading: depparse\n",
      "2021-08-10 17:54:05 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARSED_TEXT 1       סיפן\n",
      "2      נולדה\n",
      "3          ב\n",
      "4         25\n",
      "5          ב\n",
      "6     אוגוסט\n",
      "7       1997\n",
      "8       לפני\n",
      "9          ה\n",
      "10     ספירה\n",
      "11         ו\n",
      "12    התחילה\n",
      "13     ללמוד\n",
      "14         ב\n",
      "15    טכניון\n",
      "16         ב\n",
      "17       שנת\n",
      "18      2015\n",
      "Name: text, dtype: object <class 'pandas.core.series.Series'>\n",
      "{\n",
      "סיפן: Info(head=1, pos='PROPN')\n",
      "נולדה: Info(head=-1, pos='VERB')\n",
      "ב: Info(head=16, pos='ADP')\n",
      "25: Info(head=1, pos='NUM')\n",
      "אוגוסט: Info(head=3, pos='PROPN')\n",
      "1997: Info(head=5, pos='NUM')\n",
      "לפני: Info(head=9, pos='ADP')\n",
      "ה: Info(head=9, pos='DET')\n",
      "ספירה: Info(head=1, pos='NOUN')\n",
      "ו: Info(head=11, pos='CCONJ')\n",
      "התחילה: Info(head=1, pos='VERB')\n",
      "ללמוד: Info(head=11, pos='VERB')\n",
      "טכניון: Info(head=12, pos='NOUN')\n",
      "שנת: Info(head=12, pos='NOUN')\n",
      "2015: Info(head=16, pos='NUM')\n",
      "}\n",
      "\n",
      "[('סיפן נולדה', 'PER'), ('ב', 'O'), ('25', 'O'), ('ב', 'GPE'), ('אוגוסט', 'O'), ('1997', 'O'), ('לפני', 'O'), ('ה', 'GPE')]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-411adc731334>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_text_by_ner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclustered_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0minteresting_words_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_interesting_words_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_interesting_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minteresting_words_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_info_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minteresting_words_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-55233c7cf0f0>\u001b[0m in \u001b[0;36mget_interesting_words_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m         interesting_words = list(filter(lambda ner_word: \n\u001b[1;32m     71\u001b[0m                                         self._is_word_interesting(ner_word[1]), self.clustered_text))\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0minteresting_roots\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_info_for_word_cluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minteresting_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minteresting_roots\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-55233c7cf0f0>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     70\u001b[0m         interesting_words = list(filter(lambda ner_word: \n\u001b[1;32m     71\u001b[0m                                         self._is_word_interesting(ner_word[1]), self.clustered_text))\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0minteresting_roots\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_info_for_word_cluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minteresting_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minteresting_roots\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-55233c7cf0f0>\u001b[0m in \u001b[0;36m_get_info_for_word_cluster\u001b[0;34m(self, word_cluster)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_info_for_word_cluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_cluster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_cluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_verb_root\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;31m#         if self.is_verb(root):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m#             root = get_verb_name_for_verb(root)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-55233c7cf0f0>\u001b[0m in \u001b[0;36mfind_verb_root\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_verb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_word_in_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-55233c7cf0f0>\u001b[0m in \u001b[0;36mget_word_in_index\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_word_in_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_verb_root\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "text=\"\"\"\n",
    "אלברט איינשטיין נולד בגרמניה וגר בשוויץ\n",
    "\"\"\"\n",
    "text = 'סיפן נולדה ב25 באוגוסט 1997 לפני הספירה והתחילה ללמוד בטכניון בשנת 2015'\n",
    "tree = SemanticTree(text)\n",
    "tree.parse_text()\n",
    "print(tree)\n",
    "# word = 'שוויץ'\n",
    "# print('{} -> {}'.format(word, tree.find_verb_root(word)))\n",
    "# word = 'גרמניה'\n",
    "# print('{} -> {}'.format(word, tree.find_verb_root(word)))\n",
    "tree.build_ner_for_text(['B-PER', 'E-PER', 'O', 'O', 'S-GPE', 'O^O', 'O', 'O', 'S-GPE'])\n",
    "tree.add_date_tags()\n",
    "tree.cluster_text_by_ner()\n",
    "print(tree.clustered_text)\n",
    "interesting_words_info = tree.get_interesting_words_info()\n",
    "print(str_interesting_words(interesting_words_info))\n",
    "print(tree.build_info_dict(interesting_words_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8817dcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_results = ['B-PER', 'E-PER', 'O', 'O', 'S-GPE', 'O^O', 'O', 'O', 'S-GPE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "3362474e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-17 22:37:12 INFO: Loading these models for language: he (Hebrew):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | htb     |\n",
      "| mwt       | htb     |\n",
      "| pos       | htb     |\n",
      "| lemma     | htb     |\n",
      "| depparse  | htb     |\n",
      "=======================\n",
      "\n",
      "2021-06-17 22:37:12 INFO: Use device: cpu\n",
      "2021-06-17 22:37:12 INFO: Loading: tokenize\n",
      "2021-06-17 22:37:12 INFO: Loading: mwt\n",
      "2021-06-17 22:37:12 INFO: Loading: pos\n",
      "2021-06-17 22:37:12 INFO: Loading: lemma\n",
      "2021-06-17 22:37:12 INFO: Loading: depparse\n",
      "2021-06-17 22:37:13 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('אברהם נדל', 'PER'), ('עבד', 'O'), ('ב', 'O'), ('חברת', 'O'), ('אגד', 'ORG'), ('כנהג', 'O'), ('אוטובוס', 'O'), ('ב', 'O'), ('ישראל', 'GPE')]\n",
      "שם: אברהם נדל\n",
      "ארגון - עבודה: אגד\n",
      "מדינה - עבודה: ישראל\n"
     ]
    }
   ],
   "source": [
    "text=\"\"\"\n",
    "אברהם נדל עבד בחברת אגד כנהג אוטובוס בישראל\n",
    "\"\"\"\n",
    "tree = SemanticTree(text)\n",
    "tree.parse_text()\n",
    "# print(tree.tree)\n",
    "# word = 'שוויץ'\n",
    "# print('{} -> {}'.format(word, tree.find_verb_root(word)))\n",
    "# word = 'גרמניה'\n",
    "# print('{} -> {}'.format(word, tree.find_verb_root(word)))\n",
    "tree.build_ner_for_text(['B-PER', 'E-PER', 'O', 'O', 'O', 'B-ORG', 'O^O', 'O^O', 'O', 'O^S-GPE'])\n",
    "tree.cluster_text_by_ner()\n",
    "print(tree.clustered_text)\n",
    "interesting_words_info = tree.get_interesting_words_info()\n",
    "for title, word in tree.build_info_dict(interesting_words_info).items():\n",
    "    print(title + ': ' + word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "485dea03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 15:35:25 INFO: Loading these models for language: he (Hebrew):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | htb     |\n",
      "| mwt       | htb     |\n",
      "| pos       | htb     |\n",
      "| lemma     | htb     |\n",
      "| depparse  | htb     |\n",
      "=======================\n",
      "\n",
      "2021-06-19 15:35:25 INFO: Use device: cpu\n",
      "2021-06-19 15:35:25 INFO: Loading: tokenize\n",
      "2021-06-19 15:35:25 INFO: Loading: mwt\n",
      "2021-06-19 15:35:25 INFO: Loading: pos\n",
      "2021-06-19 15:35:25 INFO: Loading: lemma\n",
      "2021-06-19 15:35:25 INFO: Loading: depparse\n",
      "2021-06-19 15:35:26 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('אלברט איינשטיין', 'PER'), ('זכה', 'O'), ('ב', 'O'), ('פרס נובל', 'PRI')]\n",
      "שם: אלברט איינשטיין\n",
      "פרס - זכה: פרס נובל\n"
     ]
    }
   ],
   "source": [
    "text=\"\"\"\n",
    "אלברט איינשטיין זכה ב פרס נובל\n",
    "\"\"\"\n",
    "tree = SemanticTree(text)\n",
    "tree.parse_text()\n",
    "# print(tree.tree)\n",
    "# word = 'שוויץ'\n",
    "# print('{} -> {}'.format(word, tree.find_verb_root(word)))\n",
    "# word = 'גרמניה'\n",
    "# print('{} -> {}'.format(word, tree.find_verb_root(word)))\n",
    "tree.build_ner_for_text(['B-PER', 'E-PER', 'O', 'O', 'B-PRI', 'E-PRI'])\n",
    "tree.cluster_text_by_ner()\n",
    "print(tree.clustered_text)\n",
    "interesting_words_info = tree.get_interesting_words_info()\n",
    "for title, word in tree.build_info_dict(interesting_words_info).items():\n",
    "    print(title + ': ' + word)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hebnlp",
   "language": "python",
   "name": "hebnlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
